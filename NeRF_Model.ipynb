{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71a232f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b4b29e-f2b4-47c8-8faa-b50a2f1ff41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6e3825-8ed3-4ec0-b641-917276ebc144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Device check\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06f2a6b-71bb-4eae-a49e-5ab0d67efa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "with open('nerf_datasets/training_data.pkl', 'rb') as f:\n",
    "    training_dataset = torch.from_numpy(np.load(f, allow_pickle=True)).to(device)\n",
    "\n",
    "with open('nerf_datasets/testing_data.pkl', 'rb') as f:\n",
    "    testing_dataset = torch.from_numpy(np.load(f, allow_pickle=True)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23defe32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16000000, 9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d851a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000000, 9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32f79d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0538,  3.8455,  1.2081,  0.3340, -0.9418,  0.0390,  1.0000,  1.0000,\n",
       "         1.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset[0] #ray origins(0-2), directions(3-5), and ground truth pixel values(6-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ede23adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ray origins, directions, and ground truth pixel values\n",
    "rays_train = training_dataset[:, :6]\n",
    "rgbs_train = training_dataset[:, 6:]\n",
    "\n",
    "rays_test = testing_dataset[:, :6]\n",
    "rgbs_test = testing_dataset[:, 6:]\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(rays_train, rgbs_train)\n",
    "test_dataset = TensorDataset(rays_test, rgbs_test)\n",
    "loader = DataLoader(train_dataset, batch_size=8192, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81167f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87839515",
   "metadata": {},
   "source": [
    "# NERF Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c46b89b0-4ad0-4cce-96f4-2079165a310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture with less layers (NEED NOT RUN)\n",
    "class VolumetricRendererModel(nn.Module):\n",
    "    def __init__(self, pos_encoding_dim=10, dir_encoding_dim=4, hidden_layer_size=128):\n",
    "        super(VolumetricRendererModel, self).__init__()\n",
    "\n",
    "        self.layer_block1 = nn.Sequential(\n",
    "            nn.Linear(pos_encoding_dim * 6 + 3, hidden_layer_size), nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size), nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size), nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer_block2 = nn.Sequential(\n",
    "            nn.Linear(pos_encoding_dim * 6 + hidden_layer_size + 3, hidden_layer_size), nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size), nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size), nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size + 1)\n",
    "        )\n",
    "        \n",
    "        self.layer_block3 = nn.Sequential(\n",
    "            nn.Linear(dir_encoding_dim * 6 + hidden_layer_size + 3, hidden_layer_size // 2), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer_block4 = nn.Sequential(\n",
    "            nn.Linear(hidden_layer_size // 2, 3),nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_position(x, levels):\n",
    "        encoded = [x]\n",
    "        for i in range(levels):\n",
    "            encoded.append(torch.sin(2 ** i * x))\n",
    "            encoded.append(torch.cos(2 ** i * x))\n",
    "        return torch.cat(encoded, dim=1)\n",
    "    \n",
    "    \n",
    "    def forward(self, ray_origin, ray_direction):\n",
    "        encoded_origin = self.encode_position(ray_origin, 10)\n",
    "        encoded_direction = self.encode_position(ray_direction, 4)\n",
    "        intermediate = self.layer_block1(encoded_origin)\n",
    "        density_output = self.layer_block2(torch.cat((intermediate, encoded_origin), dim=1))\n",
    "        intermediate, density = density_output[:, :-1], nn.ReLU()(density_output[:, -1])\n",
    "        intermediate = self.layer_block3(torch.cat((intermediate, encoded_direction), dim=1))\n",
    "        color_output = self.layer_block4(intermediate)\n",
    "        return color_output, density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06905b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6abdc6ad",
   "metadata": {},
   "source": [
    "# Using NeRF model to compute integrated colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f06920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transmittance(alphas):\n",
    "    \"\"\"\n",
    "    Computes the accumulated transmittance for each sample along the ray.\n",
    "    \"\"\"\n",
    "    transmittance = torch.cumprod(alphas, dim=1)\n",
    "    return torch.cat((torch.ones((transmittance.shape[0], 1), device=alphas.device), transmittance[:, :-1]), dim=-1)\n",
    "\n",
    "def process_rays(model, origins, directions, near_plane=0, far_plane=0.5, num_bins=192):\n",
    "    \"\"\"\n",
    "    Samples points along rays and uses a NeRF model to compute integrated colors.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The NeRF model to predict RGB colors and densities.\n",
    "        origins (torch.Tensor): Ray origins [batch_size, 3].\n",
    "        directions (torch.Tensor): Ray directions [batch_size, 3].\n",
    "        near_plane (float): The starting distance for sampling along the ray.\n",
    "        far_plane (float): The ending distance for sampling along the ray.\n",
    "        num_bins (int): The number of sample points along each ray.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Integrated color along each ray.\n",
    "    \"\"\"\n",
    "    device = origins.device\n",
    "    \n",
    "    # Generate depth samples along each ray\n",
    "    t_samples = torch.linspace(near_plane, far_plane, num_bins, device=device).expand(origins.shape[0], num_bins)\n",
    "    delta = torch.cat((t_samples[:, 1:] - t_samples[:, :-1], torch.tensor([1e10], device=device).expand(origins.shape[0], 1)), dim=-1)\n",
    "\n",
    "    # Compute 3D points along each ray\n",
    "    points = origins.unsqueeze(1) + t_samples.unsqueeze(2) * directions.unsqueeze(1)\n",
    "    \n",
    "    # Predict colors and densities from the model\n",
    "    rgb_colors, densities = model(points.reshape(-1, 3), directions.expand(num_bins, directions.shape[0], 3).transpose(0, 1).reshape(-1, 3))\n",
    "    rgb_colors = rgb_colors.reshape(origins.shape[0], num_bins, 3)\n",
    "    densities = densities.reshape(origins.shape[0], num_bins)\n",
    "\n",
    "    # Calculate alphas using densities and delta\n",
    "    alphas = 1 - torch.exp(-densities * delta)\n",
    "    \n",
    "    # Calculate weights for each sample based on transmittance and alpha\n",
    "    weights = compute_transmittance(1 - alphas).unsqueeze(2) * alphas.unsqueeze(2)\n",
    "    \n",
    "    # Integrate colors along the ray by summing weighted colors\n",
    "    integrated_color = (weights * rgb_colors).sum(dim=1)\n",
    "\n",
    "    return integrated_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70819839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf270a21",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edbf7615-332f-4355-a1fc-3bda2f6d3217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, data_loader, epochs=5, near_plane=2, far_plane=6):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"start epoch: \", {epoch})\n",
    "        for rays, rgbs in data_loader:\n",
    "            origins = rays[:, :3].to(device)\n",
    "            directions = rays[:, 3:6].to(device)\n",
    "            true_colors = rgbs.to(device)\n",
    "\n",
    "            generated_colors = process_rays(model, origins, directions, near_plane, far_plane)\n",
    "            loss = ((true_colors - generated_colors) ** 2).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d782f76-107e-4d53-ace1-b3f1f320dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise mode, Optimizer, and Scheduler\n",
    "model = VolumetricRendererModel().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2, 4, 8], gamma=0.5)\n",
    "\n",
    "training_loss = train_model(model, optimizer, scheduler, loader, epochs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "plt.plot(training_loss)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef707a1",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'VRM_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5761b8",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "# model = NerfModel(hidden_dim=256)  # Ensure this matches the saved model's architecture\n",
    "model = VolumetricRendererModel(pos_encoding_dim=10, dir_encoding_dim=4, hidden_layer_size=128)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('VRM_model_105.pth', map_location=device))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "print(\"Model loaded successfully and set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ee678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "887dae21",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d02ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test DataLoader if not defined\n",
    "test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f28ba-615d-454f-a3c0-63a359a97dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for rays, rgbs in data_loader:\n",
    "            origins = rays[:, :3].to(device)\n",
    "            directions = rays[:, 3:6].to(device)\n",
    "            true_colors = rgbs.to(device)\n",
    "            predicted_colors = process_rays(model, origins, directions)\n",
    "\n",
    "            loss = ((true_colors - predicted_colors) ** 2).mean()\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "    \n",
    "    average_loss = total_loss / count\n",
    "    return average_loss\n",
    "\n",
    "# Test the model\n",
    "test_loss = evaluate_model(model, test_loader)\n",
    "print(f\"Average Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77529736",
   "metadata": {},
   "source": [
    "# Display the Model Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa305e5-b8f8-431d-851f-411c48c5bc68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_results(model, data_loader, image_size=(400, 400), batch_size=4096, num_samples=1):\n",
    "    \"\"\"\n",
    "    Processes rays in smaller batches and accumulates them to display a full image.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The NeRF model.\n",
    "        data_loader (DataLoader): DataLoader for test dataset.\n",
    "        image_size (tuple): The target image resolution (height, width).\n",
    "        batch_size (int): The batch size for processing rays.\n",
    "        num_samples (int): Number of images to display.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    H, W = image_size  # Unpack height and width\n",
    "    num_rays = H * W   # Total number of rays in one image\n",
    "\n",
    "    with torch.no_grad():\n",
    "        accumulated_true_colors = []\n",
    "        accumulated_predicted_colors = []\n",
    "        rays_accumulated = 0  # Counter for accumulated rays\n",
    "        images_displayed = 0  # Counter for the number of images displayed\n",
    "\n",
    "        for i, (rays, rgbs) in enumerate(data_loader):\n",
    "            # Check if weâ€™ve displayed the required number of images\n",
    "            if images_displayed >= num_samples:\n",
    "                break\n",
    "            \n",
    "            origins = rays[:, :3].to(device)\n",
    "            directions = rays[:, 3:6].to(device)\n",
    "            true_colors = rgbs.to(device)\n",
    "            predicted_colors = process_rays(model, origins, directions)\n",
    "            \n",
    "            # Accumulate the rays and colors\n",
    "            accumulated_true_colors.append(true_colors)\n",
    "            accumulated_predicted_colors.append(predicted_colors)\n",
    "            rays_accumulated += true_colors.shape[0]\n",
    "\n",
    "            # Debugging output to track accumulation\n",
    "#             print(f\"Batch {i+1}: Accumulated rays = {rays_accumulated}/{num_rays}\")\n",
    "\n",
    "            # Once we have enough rays to form a full image\n",
    "            if rays_accumulated >= num_rays:\n",
    "                # Concatenate accumulated results and reshape\n",
    "                full_true_colors = torch.cat(accumulated_true_colors, dim=0)[:num_rays].view(H, W, 3).cpu()\n",
    "                full_predicted_colors = torch.cat(accumulated_predicted_colors, dim=0)[:num_rays].view(H, W, 3).cpu()\n",
    "\n",
    "                # Display the full image\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(full_true_colors)\n",
    "                plt.title(\"Ground Truth\")\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(full_predicted_colors)\n",
    "                plt.title(\"Predicted\")\n",
    "                plt.show()\n",
    "                \n",
    "                # Reset accumulators for the next image\n",
    "                accumulated_true_colors = []\n",
    "                accumulated_predicted_colors = []\n",
    "                rays_accumulated = 0\n",
    "                images_displayed += 1  # Increment image display counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DataLoader with batch_size=4096\n",
    "test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=False)\n",
    "\n",
    "# Display some results\n",
    "display_results(model, test_loader, image_size=(400, 400), batch_size=4096, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74784ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f80ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
